{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKUI7UpjS1sw",
    "outputId": "49167ff1-4a55-4920-d153-f753d1ffe470"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# data = pd.read_excel('/content/gdrive/MyDrive/SpotifyRegression/datos_merged_1986_2023.xlsx')\n",
    "\n",
    "data = pd.read_excel('datos_merged_1986_2023.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tx2FLZqih-9A"
   },
   "outputs": [],
   "source": [
    "# Columns to keep\n",
    "columns_to_keep = ['popularity', 'danceability', 'year', 'duration_min', 'valence', 'speechiness', 'loudness', 'energy', 'principal_artist_followers']\n",
    "\n",
    "# Drop columns not present in the columns_to_keep list\n",
    "columns_to_drop = [col for col in data.columns if col not in columns_to_keep]\n",
    "data = data.drop(columns=columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o2mPHB0XmH4Y"
   },
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)  # Dropping rows with missing values for simplicity\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = data.drop('popularity', axis=1)  # Features\n",
    "y = data['popularity']  # Target variable\n",
    "\n",
    "columns_to_handle_outliers = ['danceability', 'year', 'duration_min', 'valence', 'speechiness', 'loudness', 'energy', 'principal_artist_followers']\n",
    "# columns_to_handle_outliers = ['valence', 'energy', 'principal_artist_followers']\n",
    "\n",
    "# Function to handle outliers using IQR method\n",
    "def handle_outliers_iqr(df, columns):\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Filtering values outside the IQR range\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Handle outliers for selected columns\n",
    "data = handle_outliers_iqr(data, columns_to_handle_outliers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-TvPukJYoj6I"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'data' contains your processed DataFrame\n",
    "\n",
    "# Separating target variable and features\n",
    "X = data.drop('danceability', axis=1)  # Features\n",
    "y = data['danceability']  # Target variable\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a_UxHKLwqeCz",
    "outputId": "7f07d498-0c0e-4a23-b3a3-00a47cd999fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'fit_intercept': True}\n",
      "R2 Score: 0.27883314941565585\n",
      "Mean Squared Error: 0.016379727424861174\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Assuming 'X_train', 'X_test', 'y_train', 'y_test' are already defined from previous steps\n",
    "\n",
    "# Outlier handling - using RobustScaler for outlier-resistant scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Creating a Linear Regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'fit_intercept': [True, False],\n",
    "    # Other hyperparameters can be added based on the available options\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='r2')  # Change scoring as needed\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best parameters found by GridSearchCV\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Evaluate the model with the best parameters on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Model Evaluation\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbrbNFnfrk-z",
    "outputId": "e52b3c72-c998-44be-e42d-0cac82a0dd13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09085290482076638\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          45       0.00      0.00      0.00         0\n",
      "          46       0.00      0.00      0.00         5\n",
      "          47       0.40      0.29      0.33         7\n",
      "          48       0.00      0.00      0.00         4\n",
      "          49       0.20      0.12      0.15        16\n",
      "          50       0.14      0.12      0.13        16\n",
      "          51       0.33      0.40      0.36        10\n",
      "          52       0.00      0.00      0.00        16\n",
      "          53       0.25      0.19      0.22        21\n",
      "          54       0.31      0.29      0.30        34\n",
      "          55       0.24      0.22      0.23        37\n",
      "          56       0.24      0.23      0.24        39\n",
      "          57       0.16      0.16      0.16        51\n",
      "          58       0.15      0.17      0.16        59\n",
      "          59       0.08      0.08      0.08        53\n",
      "          60       0.09      0.11      0.10        57\n",
      "          61       0.14      0.13      0.13        60\n",
      "          62       0.02      0.03      0.03        58\n",
      "          63       0.11      0.10      0.11        68\n",
      "          64       0.08      0.09      0.08        55\n",
      "          65       0.04      0.04      0.04        68\n",
      "          66       0.13      0.13      0.13        60\n",
      "          67       0.05      0.05      0.05        65\n",
      "          68       0.16      0.14      0.15        65\n",
      "          69       0.08      0.08      0.08        63\n",
      "          70       0.02      0.02      0.02        62\n",
      "          71       0.03      0.03      0.03        67\n",
      "          72       0.05      0.06      0.06        51\n",
      "          73       0.04      0.04      0.04        45\n",
      "          74       0.11      0.08      0.10        59\n",
      "          75       0.00      0.00      0.00        50\n",
      "          76       0.09      0.11      0.10        38\n",
      "          77       0.00      0.00      0.00        35\n",
      "          78       0.03      0.03      0.03        31\n",
      "          79       0.03      0.03      0.03        34\n",
      "          80       0.07      0.07      0.07        28\n",
      "          81       0.09      0.06      0.08        31\n",
      "          82       0.08      0.08      0.08        26\n",
      "          83       0.04      0.05      0.05        20\n",
      "          84       0.12      0.04      0.06        25\n",
      "          85       0.00      0.00      0.00         8\n",
      "          86       0.00      0.00      0.00         5\n",
      "          87       0.00      0.00      0.00         4\n",
      "          88       0.00      0.00      0.00         3\n",
      "          89       0.00      0.00      0.00         3\n",
      "          90       0.00      0.00      0.00         0\n",
      "          91       0.00      0.00      0.00         1\n",
      "          92       0.25      1.00      0.40         1\n",
      "          93       0.00      0.00      0.00         2\n",
      "          94       0.00      0.00      0.00         1\n",
      "          97       0.00      0.00      0.00         0\n",
      "          99       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.09      1618\n",
      "   macro avg       0.09      0.09      0.08      1618\n",
      "weighted avg       0.09      0.09      0.09      1618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrashertelendy/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrashertelendy/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrashertelendy/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrashertelendy/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrashertelendy/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/andrashertelendy/miniforge3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Assuming 'data' contains your processed DataFrame\n",
    "\n",
    "# Separating target variable and features\n",
    "X = data.drop('popularity', axis=1)  # Features\n",
    "y = data['popularity']  # Target variable\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Decision Tree Classifier model\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErBEbaWOsS3w",
    "outputId": "9a515ece-1d6a-49dc-d176-c24e15009dd5"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Creating and training the SVM model\n",
    "model = SVC(kernel='linear', random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2 Score:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m      5\u001b[0m logistic_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mlogistic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logistic_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     10\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1216\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m   1208\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1209\u001b[0m     X,\n\u001b[1;32m   1210\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39msolver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1215\u001b[0m )\n\u001b[0;32m-> 1216\u001b[0m \u001b[43mcheck_classification_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n\u001b[1;32m   1219\u001b[0m multi_class \u001b[38;5;241m=\u001b[39m _check_multi_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class, solver, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/sklearn/utils/multiclass.py:216\u001b[0m, in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    208\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    215\u001b[0m ]:\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown label type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Maybe you are trying to fit a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassifier, which expects discrete classes on a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression target with continuous values.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown label type: continuous. Maybe you are trying to fit a classifier, which expects discrete classes on a regression target with continuous values."
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
